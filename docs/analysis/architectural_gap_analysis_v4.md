# NNRT Architectural Gap Analysis
## Date: 2026-01-14
## Status: ðŸ”´ CRITICAL â€” Requires Architectural Rethink

---

## Executive Summary

NNRT v1 successfully performs **lexical neutralization** (softening inflammatory language) but fails at **structural decomposition** (separating observation from interpretation). 

The current output is a **cautious paraphrase**, not a **neutral representation**.

This is not a bug â€” it's a paradigm error in the core design.

---

## The Core Promise (What NNRT Should Do)

NNRT must:
1. **Extract structure** â€” Break narrative into atomic statements
2. **Separate layers** â€” Observation / Claim / Interpretation / Uncertainty
3. **Make ambiguity explicit** â€” Don't resolve it, mark it
4. **Preserve provenance** â€” Link interpretations to their source observations
5. **Enable reviewer clarity** â€” Reviewer must be able to answer:
   - What was observed?
   - What is claimed?
   - What is inferred?
   - What is unknown?

---

## What NNRT v1 Actually Does

1. **Rewrites prose linearly** â€” Preserves narrative flow
2. **Swaps insults for hedges** â€” "brutal" â†’ (removed), "wanted to" â†’ "appeared to"
3. **Keeps accusations embedded in prose** â€” Still reads as accusation
4. **Smuggles judgment through hedging language** â€” "appeared", "described as", "suggests"
5. **Produces emotionally legible output** â€” Should feel boring, clinical, flat

**Verdict**: Cosmetic neutralization, not structural normalization.

---

## Specific Failures Identified

### 1. Massive Overuse of "Appeared / Described / Suggests"

Examples from stress test output:
- "appeared to be looking for trouble"
- "described as excessive force"  
- "suggests there's a large disputed process"
- "appeared they were discussing how to cover up"

**Problem**: This launders inference through hedging verbs. The interpretation is still presented inline as fact-adjacent prose, just softened.

**Correct Behavior**:
```
Observation: Officer twisted arm behind back.
Claim: Reporter states force was excessive.
Interpretation: Reporter believes intent was to cause harm.
```

### 2. Illegal Intent/Mental State Inference

Even hedged, these are intent attributions:
- "appeared to be looking for trouble"
- "appeared to enjoy my suffering"
- "was a threat that they would fabricate charges"

**NNRT must not infer intent** â€” only label that the reporter is inferring intent.

### 3. Failure to Separate Observation vs Interpretation

**Input**: "Officer Jenkins deliberately grabbed my left arm with excessive force"

**Current Output**: "grabbed my left arm with described as excessive force"

**Correct Output**:
```
Observation: Officer Jenkins grabbed reporter's left arm and twisted it behind back.
Claim: Reporter states force caused pain.
Interpretation: Reporter believes force was excessive.
```

### 4. Introduction of New Semantic Artifacts

NNRT invented new content:
- "large disputed process" (not in input)
- "internal review and disputed process" (not in input)
- "known offender" (not in input)

**NNRT must not introduce new semantic content.**

### 5. Syntactic Degradation

Trust-killing errors:
- "grabbed my left arm with described as excessive force"
- "making physical contact with an person"
- Inconsistent tense
- Sentence fragments

---

## Architectural Gap Analysis

### Current Pipeline (v1)

```
Input Narrative
    â†“
[Segmentation] â†’ Sentences
    â†“
[Span Tagging] â†’ Identify entities, quotes, etc.
    â†“
[Policy Evaluation] â†’ Match rules, create transformations
    â†“
[Rendering] â†’ Apply transformations to prose
    â†“
Output: Softened Prose â† WRONG OUTPUT TYPE
```

### Required Pipeline (v2)

```
Input Narrative
    â†“
[Segmentation] â†’ Sentences (quote-aware) âœ… Done
    â†“
[Decomposition] â†’ Atomic statements â† NEW PASS
    â†“
[Classification] â†’ Type each statement â† NEW PASS
    â”‚   - observation
    â”‚   - claim
    â”‚   - interpretation
    â”‚   - reported_speech
    â”‚   - unknown
    â†“
[Entity/Provenance Linking] â†’ Connect statements â† NEW PASS
    â†“
[Structured IR] â†’ Statement graph with metadata
    â†“
[Optional: Prose Rendering] â†’ Only if requested, always secondary
    â†“
Output: Structured Statements + (optional) Derived Prose
```

### Key Insight: Information Loss

Current pipeline loses critical information:
- **Classification data** (observation vs interpretation) â€” never extracted
- **Provenance** (which interpretation comes from which observation) â€” never tracked
- **Confidence levels** â€” never computed
- **Source attribution** â€” partially tracked but not exposed

We need observability into the pipeline to debug this. A `--debug` flag should expose the IR at each stage.

---

## Open Questions

1. **Where does classification data live?**
   - In the rendered output? (verbose)
   - In metadata only? (hidden from default view)
   - In structured IR? (requires schema change)

2. **What is the primary output format?**
   - Structured statements (JSON/YAML)?
   - Prose with inline markers?
   - Separate layers (prose + metadata)?

3. **How do we handle ambiguity?**
   - Mark it explicitly?
   - Split into multiple possible interpretations?
   - Flag for human review?

4. **How do we test structural correctness?**
   - Current tests check string presence/absence
   - Need tests that verify statement classification
   - Need tests that verify provenance links

---

## Proposed Next Steps

### Phase 1: Design (Before Any Code)
- [ ] Define NNRT output schema formally
- [ ] Create manual "gold standard" decomposition of stress test paragraph
- [ ] Define invariants that must hold (testable properties)
- [ ] Design debug/observability mode

### Phase 2: Infrastructure  
- [ ] Add `--debug` flag to expose IR at each stage
- [ ] Create statement classification pass
- [ ] Create provenance linking pass
- [ ] Update IR schema to support structured statements

### Phase 3: Implementation
- [ ] Implement decomposition pass
- [ ] Implement classification pass
- [ ] Update rendering to be optional/secondary
- [ ] Create new stress tests for structural correctness

### Phase 4: Validation
- [ ] Manual review of stress test output
- [ ] Adversarial tests that current system would fail
- [ ] Reviewer clarity test (can reviewer distinguish observation/claim/interpretation?)

---

## What We Fixed Today (Still Valid)

The following fixes are still valuable and should be preserved:

1. âœ… **Quote-aware segmentation** â€” Multi-sentence quotes now stay together
2. âœ… **Overlapping rule consumption** â€” No more word corruption
3. âœ… **Verb tense preservation** â€” "screamed" â†’ "spoke loudly" (past tense correct)
4. âœ… **Article agreement** â€” "a alleged" â†’ "an alleged"
5. âœ… **Double verb fix** â€” "was clearly looking" no longer becomes "was appeared"

These are valid improvements to the lexical layer. The problem is that lexical improvement alone is insufficient.

---

## Key Realization

> NNRT is supposed to break the story apart, not retell it politely.

The tool isn't bad. It's incomplete. We've built a good lexical transformer. We haven't built a structural decomposer yet.

The next phase is about **decomposition**, not **rewriting**.

---

## References

- `stress_tests/cases/02_mega_law_enforcement.yaml` â€” Full stress test
- `docs/analysis/infrastructure_gaps_v3.md` â€” Previous gap analysis (lexical level)
- This document â€” Architectural gap analysis (structural level)
